{"cells":[{"cell_type":"markdown","metadata":{"id":"qscWzBX9BYZD"},"source":["# Neural Networks in PyTorch - Example on MNIST Handwritten Digit Classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvPr7tQIBYZE"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","\n","# Set random seed for reproducibility\n","torch.manual_seed(302)\n","np.random.seed(302)\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,)) # this ensures that the mean pixel value is 0 and the standard deviation is 1\n","])\n","\n","train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n","\n","print(train_dataset); print(test_dataset)\n","\n","# Creating data loaders\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n"]},{"cell_type":"markdown","metadata":{"id":"O1B1SMTZBYZF"},"source":["## Explorative Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJOBiEFIBYZJ"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","\n","image_batch, label_batch = next(iter(train_loader))\n","image_batch.shape, label_batch.shape\n","\n","print(label_batch[:9])\n","\n","plt.figure(figsize=(10, 10))\n","plt.imshow(image_batch[:9, 0, :, :]\n","           .reshape(3, 3, 28, 28)\n","           .permute(0, 2, 1, 3)\n","           .reshape(28*3, 28*3), cmap='gray')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wvn3yCU6BYZQ"},"source":["## Defining the Network"]},{"cell_type":"markdown","metadata":{"id":"Wba7vNQ9BYZR"},"source":["Feed-forward neural network with 3 hidden layers in PyTorch:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zU2qiq_2BYZR"},"outputs":[],"source":["class NeuralNet(nn.Module):\n","    def __init__(self):\n","      super(NeuralNet, self).__init__()\n","      self.fc1 = nn.Linear(in_features=28*28, out_features=128)\n","      self.fc2 = nn.Linear(in_features=128, out_features=128)\n","      self.fc3 = nn.Linear(in_features=128, out_features=10)\n","\n","\n","    def forward(self, x):\n","      # Flatten the data (B, 1, 28, 28) => (B, 784), where B is the batch size\n","      x = torch.flatten(x, start_dim=1)\n","\n","      # Pass data through 1st fully connected layer\n","      x = self.fc1(x)\n","      # Apply ReLU non-linearity\n","      x = F.relu(x)\n","\n","      # Pass data through 2nd fully connected layer\n","      x = self.fc2(x)\n","      # Apply ReLU non-linearity\n","      x = F.relu(x)\n","\n","      # Pass data through 3rd fully connected layer\n","      x = self.fc3(x)\n","\n","      # Before passing x to the softmax function, the values in x are called *logits*.\n","      # Finally, apply softmax to x (logits)\n","      probs = F.softmax(x, dim=1)\n","\n","      return probs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtBT7qX_BYZZ"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","model = NeuralNet().to(device)\n","print(model)\n","\n","# Parameters of the model\n","for p in model.parameters():\n","    print(p.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bBJ8yPmBYZa"},"outputs":[],"source":["# Just looking into how the inputs and (untrained) model outputs would look like\n","x = image_batch[:1].to(device)\n","print(f'Input image shape: {x.shape}')\n","\n","probs = model(x)\n","print(f'Model output: {probs}')\n","\n","probs = torch.exp(probs)\n","print(probs)\n","print(torch.sum(probs))"]},{"cell_type":"markdown","metadata":{"id":"G8fFoKnBBYZi"},"source":["## Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxb_giRrBYZi"},"outputs":[],"source":["learning_rate = 0.01\n","num_epochs = 5\n","\n","model = NeuralNet().to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","losses = []\n","for epoch in range(num_epochs):\n","    print('-'*20, f'Epoch {epoch}', '-'*20)\n","    # Train one epoch\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","\n","        optimizer.zero_grad()\n","        probs = model(data)\n","        loss = loss_fn(probs, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        losses.append(loss.item())\n","        if batch_idx % 100 == 0:\n","            print(f'Train Epoch {epoch} | Loss: {loss.item()}')\n","    print(f'\\nAverage train loss in epoch {epoch}: {np.mean(losses[-len(train_loader):])}')\n","\n","    # Evaluate on test set (for monitoring training at the end of each epoch)\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            probs = model(data)\n","            test_loss += loss_fn(probs, target).item()\n","            pred = torch.argmax(probs, dim=1)  # get the index of the max probability as the predicted output\n","            correct += (pred == target).sum().item()\n","\n","    test_loss = test_loss / len(test_loader)\n","    avg_correct = correct / len(test_loader.dataset)\n","    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * avg_correct:.0f}%)\\n')"]},{"cell_type":"markdown","metadata":{"id":"WsRsdfu7BYZk"},"source":["## Plot Loss Curve"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7UQLkQIDBYZk"},"outputs":[],"source":["losses_smoothed = np.array(losses).reshape(-1, 10).mean(axis=1) # average every 10 batch losses\n","steps = np.arange(len(losses))\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(steps, losses, 'b', alpha=0.5)\n","plt.plot(steps[::10], losses_smoothed, 'b')\n","plt.title('Training Loss')\n","plt.xlabel('Iterations')\n","plt.ylabel('Loss')\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}